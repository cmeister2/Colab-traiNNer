{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-BasicSR-lightning-local.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "xREJTZSbsEa8",
        "eodC8LcPOLFe",
        "nhj7M36j3cP6",
        "BB3Rs436n0WY"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsxYKkWLXju5"
      },
      "source": [
        "# Colab-BasicSR (pytorch lightning)\n",
        "\n",
        "[This tutorial](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09), [this issue](https://stackoverflow.com/questions/65387967/misconfigurationerror-no-tpu-devices-were-found-even-when-tpu-is-connected-in)  and [this Colab](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/03-basic-gan.ipynb#scrollTo=3vKszYf6y1Vv) were very helpful. This Colab does support single-GPU, multi-GPU and TPU training. (TPU training will depend on the architecture. RFR does not work with TPUs because of .cuda() for example.)\n",
        "\n",
        "Can use various loss functions and has the context_encoder discriminator as default. Currently there are only various inpainting generators from [my BasicSR fork](https://github.com/styler00dollar/Colab-BasicSR).\n",
        "\n",
        "What is not included inside this Colab, but is included in [my normal BasicSR Colab](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab-BasicSR.ipynb):\n",
        "- [edge-informed-sisr](https://github.com/knazeri/edge-informed-sisr/blob/master/src/models.py)\n",
        "- [USRNet](https://github.com/cszn/KAIR/blob/master/models/network_usrnet.py)\n",
        "- [OFT Dataloader](https://github.com/styler00dollar/Colab-BasicSR/tree/master/codes/data)\n",
        "- Some loss functions, but most are here\n",
        "- DiffAug / Mixup\n",
        "\n",
        "What currently is here but not inside the other Colab:\n",
        "- Custom mask loading\n",
        "- New discriminators (EfficientNet, ResNeSt, Transformer)\n",
        "- [AdamP](https://github.com/clovaai/AdamP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPB6KmEOPQW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3CR1399f8_5"
      },
      "source": [
        "!git clone -b lightning https://github.com/styler00dollar/Colab-BasicSR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBmBFu3vTlTX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "# create empty folders\n",
        "!mkdir /content/hr\n",
        "!mkdir /content/lr\n",
        "!mkdir /content/val_hr\n",
        "!mkdir /content/val_lr\n",
        " \n",
        "!mkdir /content/masks\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/data\n",
        "!mkdir /content/logs/\n",
        " \n",
        "#!pip install pytorch-lightning -U\n",
        "# Hotfix, to avoid pytorch-lightning bug\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "!pip install tensorboardX\n",
        " \n",
        "# optional\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDUI7DqflrY",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "# create empty folders\n",
        "!mkdir /content/masks\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/data\n",
        "!mkdir /content/logs/\n",
        "\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "#!pip install pytorch-lightning\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n",
        "# optional\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I03TFfDaqtR",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fSZHn8ayFM",
        "cellView": "form"
      },
      "source": [
        "#@title copy data somehow\n",
        "\"\"\"\n",
        "!mkdir '/content/data'\n",
        "!mkdir '/content/data/images'\n",
        "!cp \"/content/drive/MyDrive/classification_v3.7z\" \"/content/data/images/data.7z\"\n",
        "%cd /content/data/images\n",
        "!7z x \"data.7z\"\n",
        "!rm -rf /content/data/images/data.7z\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZc7QSEN2uoR"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwqBGsjmyc1l",
        "cellView": "form"
      },
      "source": [
        "#@title config.yaml\n",
        "%%writefile /content/Colab-BasicSR/code/config.yaml\n",
        "name: template\n",
        "scale: 16\n",
        "gpus: 1\n",
        "tpu_cores: 8\n",
        "use_tpu: False\n",
        "use_amp: True\n",
        "use_swa: False\n",
        "progress_bar_refresh_rate: 20\n",
        "default_root_dir: '/content/drive/MyDrive/Colab-BasicSR/'\n",
        "\n",
        "# Dataset options:\n",
        "datasets:\n",
        "  train:\n",
        "    # DS_inpaint: hr is from dataroot_HR, loads masks\n",
        "    # DS_inpaint_tiled: hr is from dataroot_HR, but images are grids (16x16 256px currently), loads masks\n",
        "    # DS_inpaint_tiled_batch: hr is from dataroot_HR, but images are grids (16x16 256px currently) and processed as a batch (batch_size_DL), loads masks\n",
        "    # DS_lrhr: loads lr from dataroot_LR and hr from dataroot_HR\n",
        "    # DS_lrhr_batch_oft: loads grayscale-hr (3x3 400px) from dataroot_LR and generates lr by downscaling otf randomly\n",
        "\n",
        "    mode: DS_lrhr_batch_oft # DS_inpaint | DS_inpaint_tiled | DS_inpaint_tiled_batch | DS_lrhr | DS_lrhr_batch_oft\n",
        "    grayscale: True # If true, reads as 1-Channel image. If false, reads as 3-channel image. Currently only implemented for DS_lrhr_batch_oft\n",
        "    dataroot_HR: '/content/hr' # Original, with a single directory. Inpainting will use this directory as source image.\n",
        "    dataroot_LR: '/content/lr' # Original, with a single directory\n",
        "\n",
        "    n_workers: 1 # 0 to disable CPU multithreading, or an integrer representing CPU threads to use for dataloading\n",
        "    batch_size: 1\n",
        "    batch_size_DL: 9 # If a batched/tiled dataloader is used, use this batch parameter instead. Leave normal batch_size at 1.\n",
        "    HR_size: 400 # The resolution the network will get. Random crop gets applied if that resolution does not match.\n",
        "    image_channels: 1 # number of channels to load images in\n",
        "\n",
        "    masks: '/content/random_masks/train+test_inverted/'\n",
        "    max_epochs: 200\n",
        "    save_step_frequency: 5000 # also validation frequency\n",
        "\n",
        "    # batch\n",
        "    # If a tiled dataloader is used, specify image charactaristics. Random crop will not be applied. Maybe in the future.\n",
        "    image_size: 400 # Size of one tile\n",
        "    amount_tiles: 3 # Amount of tiles inside the merged grid image\n",
        "\n",
        "    # if edge data is required\n",
        "    canny_min: 100\n",
        "    canny_max: 150\n",
        "\n",
        "  val:\n",
        "    dataroot_HR: '/content/drive/MyDrive/Colab-BasicSR/val_input_hr/'\n",
        "    dataroot_LR: '/content/drive/MyDrive/Colab-BasicSR/val_input_lr/' # Inpainting will use this directory as input\n",
        "\n",
        "path:\n",
        "    pretrain_model_G: # '/content/Checkpoint_0_0_D.pth'\n",
        "    checkpoint_path: # '/content/drive/MyDrive/Colab-BasicSR/Checkpoint_31_715000.ckpt'\n",
        "    checkpoint_save_path: '/content/drive/MyDrive/Colab-BasicSR/'\n",
        "    validation_output_path: '/content/drive/MyDrive/Colab-BasicSR/val/'\n",
        "    log_path: '/content/drive/MyDrive/Colab-BasicSR/'\n",
        "\n",
        "# Generator options:\n",
        "network_G:\n",
        "    finetune: False # Important for further rfr training. Apply that after training for a while. https://github.com/jingyuanli001/RFR-Inpainting/issues/33\n",
        "\n",
        "    # ESRGAN:\n",
        "    netG: RRDB_net # RRDB_net (original ESRGAN arch) | MRRDB_net (modified/\"new\" arch)\n",
        "    norm_type: null\n",
        "    mode: CNA\n",
        "    nf: 64 # of discrim filters in the first conv layer (default: 64, good: 32)\n",
        "    nb: 23 # (default: 23, good: 8)\n",
        "    in_nc: 1 # of input image channels: 3 for RGB and 1 for grayscale\n",
        "    out_nc: 1 # of output image channels: 3 for RGB and 1 for grayscale\n",
        "    gc: 32\n",
        "    group: 1\n",
        "    convtype: Conv2D # Conv2D | PartialConv2D\n",
        "    net_act: leakyrelu # swish | leakyrelu\n",
        "    gaussian: true # true | false\n",
        "    plus: false # true | false\n",
        "    finalact: None #tanh # Test. Activation function to make outputs fit in [-1, 1] range. Default = None. Coordinate with znorm.\n",
        "    upsample_mode: 'upconv'\n",
        "    nr: 3\n",
        "\n",
        "    # ASRGAN:\n",
        "    #which_model_G: asr_resnet # asr_resnet | asr_cnn\n",
        "    #nf: 64\n",
        "\n",
        "    # PPON:\n",
        "    #netG: ppon # | ppon\n",
        "    ##norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 24\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    ##gc: 32\n",
        "    #group: 1\n",
        "    ##convtype: Conv2D #Conv2D | PartialConv2D\n",
        "\n",
        "    # SRGAN:\n",
        "    #netG: sr_resnet # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 16\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "\n",
        "    # SR:\n",
        "    #netG: RRDB_net # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 23\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    #gc: 32\n",
        "    #group: 1\n",
        "\n",
        "    # PAN:\n",
        "    # netG: pan_net\n",
        "    # in_nc: 3\n",
        "    # out_nc: 3\n",
        "    # nf: 40\n",
        "    # unf: 24\n",
        "    # nb: 16\n",
        "    # self_attention: true\n",
        "    # double_scpa: false\n",
        "\n",
        "    # edge-informed-sisr\n",
        "    #which_model_G: sisr\n",
        "    #use_spectral_norm: True\n",
        "\n",
        "    # USRNet\n",
        "    #netG: USRNet\n",
        "    #in_nc=4\n",
        "    #out_nc=3\n",
        "    #nc=[64, 128, 256, 512]\n",
        "    #nb=2\n",
        "    #act_mode='R'\n",
        "    #downsample_mode='strideconv'\n",
        "    #upsample_mode='convtranspose'\n",
        "\n",
        "    # ----Inpainting Generators----\n",
        "    # DFNet (batch_size: 2+, needs 2^x image input and validation) (2019)\n",
        "    #netG: DFNet\n",
        "    #c_img: 3\n",
        "    #c_mask: 1\n",
        "    #c_alpha: 3\n",
        "    #mode: nearest\n",
        "    #norm: batch\n",
        "    #act_en: relu\n",
        "    #act_de: leaky_relu\n",
        "    #en_ksize: [7, 5, 5, 3, 3, 3, 3, 3]\n",
        "    #de_ksize: [3, 3, 3, 3, 3, 3, 3, 3]\n",
        "    #blend_layers: [0, 1, 2, 3, 4, 5]\n",
        "    #conv_type: normal # partial | normal | deform\n",
        "    \n",
        "\n",
        "    # EdgeConnect (2019)\n",
        "    #netG: EdgeConnect\n",
        "    #use_spectral_norm: True\n",
        "    #residual_blocks_edge: 8\n",
        "    #residual_blocks_inpaint: 8\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #conv_type_inpaint: 'normal' # normal | partial | deform\n",
        "\n",
        "    # CSA (2019)\n",
        "    #netG: CSA\n",
        "    #c_img: 3\n",
        "    #norm: 'instance'\n",
        "    #act_en: 'leaky_relu'\n",
        "    #act_de: 'relu'\n",
        "\n",
        "    # RN (2020)\n",
        "    #netG: RN\n",
        "    #input_channels: 3\n",
        "    #residual_blocks: 8\n",
        "    #threshold: 0.8\n",
        "\n",
        "    # deepfillv1 (2018)\n",
        "    #netG:  deepfillv1\n",
        "\n",
        "    # deepfillv2 (2019)\n",
        "    #netG: deepfillv2\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu'\n",
        "    #norm: 'in'\n",
        "    #conv_type: partial # partial | normal\n",
        "\n",
        "    # Adaptive (2020)\n",
        "    #netG: Adaptive\n",
        "    #in_channels: 3\n",
        "    #residual_blocks: 1\n",
        "    #init_weights: True\n",
        "\n",
        "    # Global (2020)\n",
        "    #netG: Global\n",
        "    #input_dim: 5\n",
        "    #ngf: 32\n",
        "    #use_cuda: True\n",
        "    #device_ids: [0]\n",
        "\n",
        "    # Pluralistic (2019)\n",
        "    #netG: Pluralistic\n",
        "    #ngf_E: 32\n",
        "    #z_nc_E: 128\n",
        "    #img_f_E: 128\n",
        "    #layers_E: 5\n",
        "    #norm_E: 'none'\n",
        "    #activation_E: 'LeakyReLU'\n",
        "    #ngf_G: 32\n",
        "    #z_nc_G: 128\n",
        "    #img_f_G: 128\n",
        "    #L_G: 0\n",
        "    #output_scale_G: 1\n",
        "    #norm_G: 'instance'\n",
        "    #activation_G: 'LeakyReLU'\n",
        "\n",
        "    # crfill (2020)\n",
        "    #netG: crfill\n",
        "    #cnum: 48\n",
        "\n",
        "    # DeepDFNet (experimental)\n",
        "    #netG: DeepDFNet\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu'\n",
        "    #norm: 'in'\n",
        "\n",
        "    # partial (2018)\n",
        "    #netG: partial\n",
        "\n",
        "    # DMFN (2020)\n",
        "    #netG: DMFN\n",
        "    #in_nc: 4\n",
        "    #out_nc: 3\n",
        "    #nf: 64\n",
        "    #n_res: 8\n",
        "    #norm: 'in'\n",
        "    #activation: 'relu'\n",
        "\n",
        "    # pennet (2019)\n",
        "    #netG: pennet\n",
        "\n",
        "    # LBAM (2019)\n",
        "    #netG: LBAM\n",
        "    #inputChannels: 4\n",
        "    #outputChannels: 3\n",
        "\n",
        "    # RFR (use_swa: false, no TPU) (2020)\n",
        "    #netG: RFR\n",
        "    #conv_type: partial # partial | deform\n",
        "\n",
        "    # FRRN (2019)\n",
        "    #netG: FRRN\n",
        "\n",
        "    # PRVS (2019)\n",
        "    #netG: PRVS\n",
        "\n",
        "    # CRA (HR_size: 512) (2020)\n",
        "    #netG: CRA\n",
        "    #activation: 'elu'\n",
        "    #norm: 'none'\n",
        "\n",
        "    # atrous (2020)\n",
        "    #netG: atrous\n",
        "\n",
        "    # MEDFE (batch_size: 1) (2020)\n",
        "    #netG: MEDFE\n",
        "\n",
        "    # AdaFill (2021)\n",
        "    #netG: AdaFill\n",
        "\n",
        "# Discriminator options:\n",
        "network_D:\n",
        "    # VGG\n",
        "    #netD: VGG\n",
        "    #size: 256\n",
        "    #in_nc: 3 #3\n",
        "    #base_nf: 64\n",
        "    #norm_type: 'batch'\n",
        "    #act_type: 'leakyrelu'\n",
        "    #mode: 'CNA'\n",
        "    #convtype: 'Conv2D'\n",
        "    #arch: 'ESRGAN'\n",
        "\n",
        "    # VGG fea\n",
        "    #netD: VGG_fea\n",
        "    #size: 256\n",
        "    #in_nc: 3\n",
        "    #base_nf: 64\n",
        "    #norm_type: 'batch'\n",
        "    #act_type: 'leakyrelu'\n",
        "    #mode: 'CNA'\n",
        "    #convtype: 'Conv2D'\n",
        "    #arch: 'ESRGAN'\n",
        "    #spectral_norm: False\n",
        "    #self_attention: False\n",
        "    #max_pool: False\n",
        "    #poolsize: 4\n",
        "\n",
        "\n",
        "    #netD: VGG_128_SN\n",
        "\n",
        "    # VGGFeatureExtractor\n",
        "    #netD: VGGFeatureExtractor\n",
        "    #feature_layer: 34\n",
        "    #use_bn: False\n",
        "    #use_input_norm: True\n",
        "    #device: 'cpu'\n",
        "    #z_norm: False\n",
        "\n",
        "    # PatchGAN\n",
        "    #netD: NLayerDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #n_layers: 3\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "    #use_sigmoid: False\n",
        "    #getIntermFeat: False\n",
        "    #patch: True\n",
        "    #use_spectral_norm: False\n",
        "\n",
        "    # Multiscale\n",
        "    #netD: MultiscaleDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #n_layers: 3\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "    #se_sigmoid: False\n",
        "    #num_D: 3\n",
        "    #getIntermFeat: False\n",
        "\n",
        "    # ResNet\n",
        "    #netD: Discriminator_ResNet_128\n",
        "    #in_nc: 3\n",
        "    #base_nf: 64\n",
        "    #norm_type: 'batch'\n",
        "    #act_type: 'leakyrelu'\n",
        "    #mode: 'CNA'\n",
        "    \n",
        "    #netD: ResNet101FeatureExtractor\n",
        "    #use_input_norm: True\n",
        "    #device: 'cpu'\n",
        "    #z_norm: False\n",
        "\n",
        "    # MINC\n",
        "    #netD: MINCNet\n",
        "\n",
        "    # Pixel\n",
        "    #netD: PixelDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "\n",
        "    # EfficientNet (3-channel input)\n",
        "    #netD: EfficientNet\n",
        "    #EfficientNet_pretrain: 'efficientnet-b0'\n",
        "\n",
        "    # ResNeSt\n",
        "    #netD: ResNeSt\n",
        "    #ResNeSt_pretrain: 'resnest50' # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "\n",
        "    # Transformer\n",
        "    #netD: TranformerDiscriminator\n",
        "    #img_size: 256\n",
        "    #patch_size: 1\n",
        "    #in_chans: 3\n",
        "    #num_classes: 1\n",
        "    #embed_dim: 64\n",
        "    #depth: 7\n",
        "    #num_heads: 4\n",
        "    #mlp_ratio: 4.\n",
        "    #qkv_bias: False\n",
        "    #qk_scale: None\n",
        "    #drop_rate: 0.\n",
        "    #attn_drop_rate: 0.\n",
        "    #drop_path_rate: 0.\n",
        "    #hybrid_backbone: None\n",
        "    #norm_layer: nn.LayerNorm\n",
        "\n",
        "    netD: context_encoder\n",
        "\n",
        "train:\n",
        "    scheduler: AdamP # AdamP, Adam, SGDP\n",
        "    lr: 0.0001\n",
        "\n",
        "    # AdamP\n",
        "    betas0: 0.9\n",
        "    betas1: 0.999\n",
        "    weight_decay: 1e-2\n",
        "\n",
        "    # SGDP\n",
        "    weight_decay: 1e-5\n",
        "    momentum: 0.9\n",
        "    nesterov: True\n",
        "\n",
        "    # Losses:\n",
        "    L1Loss_weight: 2\n",
        "\n",
        "    # HFENLoss\n",
        "    HFEN_weight: 0\n",
        "    loss_f: L1CosineSim()\n",
        "    kernel: 'log'\n",
        "    kernel_size: 15\n",
        "    sigma: 2.5\n",
        "    norm: False\n",
        "\n",
        "    # Elastic\n",
        "    Elatic_weight: 0\n",
        "    a: 0.2\n",
        "    reduction_elastic: 'mean'\n",
        "\n",
        "    # Relative L1\n",
        "    Relative_l1_weight: 0\n",
        "    eps: .01\n",
        "    reduction_realtive: 'mean'\n",
        "\n",
        "    # L1CosineSim (3-channel input)\n",
        "    L1CosineSim_weight: 0\n",
        "    loss_lambda: 5\n",
        "    reduction_L1CosineSim: 'mean'\n",
        "\n",
        "    # ClipL1\n",
        "    ClipL1_weight: 0\n",
        "    clip_min: 0.0\n",
        "    clip_max: 10.0\n",
        "\n",
        "    # FFTLoss\n",
        "    FFTLoss_weight: 0\n",
        "    loss_f: L1Loss\n",
        "    reduction_fft: 'mean'\n",
        "\n",
        "    OFLoss_weight: 0\n",
        "\n",
        "    # GPLoss\n",
        "    GPLoss_weight: 0\n",
        "    trace: False\n",
        "    spl_denorm: False\n",
        "\n",
        "    # CPLoss\n",
        "    CPLoss_weight: 0\n",
        "    rgb: True\n",
        "    yuv: True\n",
        "    yuvgrad: True\n",
        "    trace: False\n",
        "    spl_denorm: False\n",
        "    yuv_denorm: False\n",
        "\n",
        "    StyleLoss_weight: 0\n",
        "\n",
        "    # TVLoss\n",
        "    TVLoss_weight: 0\n",
        "    tv_type: 'tv'\n",
        "    p: 1\n",
        "\n",
        "    # PerceptualLoss (only single gpu)\n",
        "    PerceptualLoss_weight: 1\n",
        "    model: 'net-lin'\n",
        "    net: 'alex'\n",
        "    colorspace: 'rgb'\n",
        "    spatial: False\n",
        "    use_gpu: True # False for TPU training\n",
        "    gpu_ids: [0]\n",
        "    #model_path: None # todo\n",
        "\n",
        "    # Contextual_Loss (3-channel input)\n",
        "    Contexual_weight: 0\n",
        "    layers_weights: {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    crop_quarter: False\n",
        "    max_1d_size: 100\n",
        "    distance_type: 'cosine'\n",
        "    b: 1.0\n",
        "    band_width: 0.5\n",
        "    use_vgg: True\n",
        "    net_contextual: 'vgg19'\n",
        "    calc_type: 'regular'\n",
        "\n",
        "    # Style (3-channel input)\n",
        "    StyleLoss_weight: 0\n",
        "\n",
        "    stage1_weight: 0 # only if the network outputs 2 images, will use l1\n",
        "\n",
        "    # Differentiable Augmentation for Data-Efficient GAN Training\n",
        "    diffaug: False # not implemented\n",
        "    policy: 'color,translation,cutout'\n",
        "\n",
        "    # Metrics\n",
        "    metrics: ['PSNR', 'MSE'] # PSNR | SSIM | AE | MSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VRy6ieDQGvt"
      },
      "source": [
        "%cd /content/Colab-BasicSR/code/\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xREJTZSbsEa8"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcHYqzCGleIX",
        "cellView": "form"
      },
      "source": [
        "#@title testing the model\n",
        "%cd /content/Colab-BasicSR/code/\n",
        "# output path currently defined in config file\n",
        "# test file for inpainting, mask with green\n",
        "!python test.py --data_input_folder '/content/val_lr/' --fp16_mode False --netG_pth_path '/content/drive/MyDrive/Colab-BasicSR/lightning/Checkpoint_39_100000_G.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eodC8LcPOLFe"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODoScLJf3zHu"
      },
      "source": [
        "# Using pillow simd instead of normal pillow. Restart runtime after that.\n",
        "!pip uninstall Pillow -y\n",
        "!pip install Pillow-SIMD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_pXMEzz5qRS",
        "cellView": "form"
      },
      "source": [
        "#@title creating tiles (with skip)\n",
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "resize_method = 'PIL' #@param [\"OpenCV\", \"PIL\"] {allow-input: false}\n",
        "grayscale = False #@param {type:\"boolean\"}\n",
        "\n",
        "rootdir = '/content/data' #@param {type:\"string\"}\n",
        "destination_dir = \"/content/out/\" #@param {type:\"string\"}\n",
        "broken_dir = '/content/opencv_fail/' #@param {type:\"string\"}\n",
        " \n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files_jpeg = glob.glob(rootdir + '/**/*.jpeg', recursive=True)\n",
        "files_webp = glob.glob(rootdir + '/**/*.webp', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "files.extend(files_jpeg)\n",
        "files.extend(files_webp)\n",
        "err_files=[]\n",
        "\n",
        "amount_tiles = 2 #@param\n",
        "image_size = 256 #@param\n",
        "\n",
        "filepos = 0\n",
        "img_cnt = 0\n",
        "filename_cnt = 0\n",
        "\n",
        "if grayscale == True:\n",
        "  tmp_img = numpy.zeros((amount_tiles*image_size,amount_tiles*image_size))\n",
        "elif grayscale == False:\n",
        "  tmp_img = numpy.zeros((amount_tiles*image_size,amount_tiles*image_size, 3))\n",
        "\n",
        "with tqdm.tqdm(files) as pbar:\n",
        "  while True:\n",
        "      if grayscale == True:\n",
        "        image = cv2.imread(files[filepos], cv2.IMREAD_GRAYSCALE)\n",
        "      elif grayscale == False:\n",
        "        image = cv2.imread(files[filepos])\n",
        "\n",
        "      filepos += 1\n",
        "\n",
        "      if image is not None:\n",
        "        \n",
        "        i = img_cnt % amount_tiles\n",
        "        j = img_cnt // amount_tiles\n",
        "\n",
        "        #####################################\n",
        "        # resize with opencv\n",
        "        if resize_method == \"OpenCV\":\n",
        "          image = cv2.resize(image, (image_size,image_size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # resize with PIL\n",
        "        elif resize_method == \"PIL\":\n",
        "          if grayscale == True:\n",
        "            image = Image.fromarray(image)\n",
        "            image = image.resize((image_size,image_size))\n",
        "            image = np.asarray(image)\n",
        "          if grayscale == False:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = Image.fromarray(image)\n",
        "            image = image.resize((image_size,image_size), resample=PIL.Image.LANCZOS)\n",
        "            image = np.asarray(image)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        #####################################\n",
        "\n",
        "        tmp_img[i*image_size:(i+1)*image_size, j*image_size:(j+1)*image_size] = image\n",
        "        img_cnt += 1\n",
        "      else:\n",
        "        print(files[filepos])\n",
        "        print(f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "        shutil.move(files[filepos], f'{broken_dir}/{os.path.basename(files[filepos])}')\n",
        "\n",
        "      if img_cnt == (amount_tiles*amount_tiles):\n",
        "        #cv2.imwrite(destination_dir+str(filename_cnt)+\".jpg\", tmp_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "        cv2.imwrite(destination_dir+str(filename_cnt)+\".png\", tmp_img)\n",
        "        filename_cnt += 1\n",
        "        img_cnt = 0\n",
        "      pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1_olG9wXux6",
        "cellView": "form"
      },
      "source": [
        "#@title convert to onnx\n",
        "#@markdown Make sure the input dimensions are correct. Maybe a runtime restart is needed if it complains about ``TypeError: forward() missing 1 required positional argument``. Make sure you only run the required cells.\n",
        "from torch.autograd import Variable\n",
        "model = CustomTrainClass()\n",
        "checkpoint_path = '/content/Checkpoint_0_0.ckpt' #@param\n",
        "output_path = '/content/output.onnx' #@param\n",
        "model = model.load_from_checkpoint(checkpoint_path) # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "dummy_input = Variable(torch.randn(1, 1, 64, 64))\n",
        "\n",
        "model.to_onnx(output_path, input_sample=dummy_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nIKT3KMqWy",
        "cellView": "form"
      },
      "source": [
        "#@title pip list with space\n",
        "!pip list | tail -n +3 | awk '{print $1}' | xargs pip show | grep -E 'Location:|Name:' | cut -d ' ' -f 2 | paste -d ' ' - - | awk '{print $2 \"/\" tolower($1)}' | xargs du -sh 2> /dev/null | sort -hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VET9bxpwqPhf",
        "cellView": "form"
      },
      "source": [
        "#@title tiling script\n",
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import os\n",
        "from multiprocessing.pool import ThreadPool as ThreadPool\n",
        "\n",
        "rootdir = 'C:\\\\Users\\\\x\\\\Desktop\\\\Untitled Folder\\\\data\\\\' #@param {type:\"string\"}\n",
        "destination_dir = \"C:\\\\Users\\\\x\\\\Desktop\\\\Untitled Folder\\\\\" #@param {type:\"string\"}\n",
        "broken_dir = 'C:\\\\Users\\\\x\\\\Desktop\\\\Untitled Folder\\\\' #@param {type:\"string\"}\n",
        "threads = 2 #@param\n",
        "tile_size = 256 #@param\n",
        "\n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files_jpeg = glob.glob(rootdir + '/**/*.jpeg', recursive=True)\n",
        "files_webp = glob.glob(rootdir + '/**/*.webp', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "files.extend(files_jpeg)\n",
        "files.extend(files_webp)\n",
        "err_files=[]\n",
        "\n",
        "pool = ThreadPool(threads)\n",
        "\n",
        "def tiling(f):\n",
        "  image = cv2.imread(f)\n",
        "  if image is not None:\n",
        "      counter = 0\n",
        "\n",
        "      x = image.shape[0]\n",
        "      y = image.shape[1]\n",
        "\n",
        "      x_amount = x // tile_size\n",
        "      y_amount = y // tile_size\n",
        "\n",
        "      for i in range(x_amount):\n",
        "        for j in range(y_amount):\n",
        "          crop = image[i*tile_size:(i+1)*tile_size, (j*tile_size):(j+1)*tile_size]\n",
        "          cv2.imwrite(os.path.join(destination_dir, os.path.splitext(os.path.basename(f))[0] + str(counter) + \".png\"), crop)\n",
        "          counter += 1\n",
        "\n",
        "    else:\n",
        "        print(f'Broken file: {os.path.basename(f)}')\n",
        "        shutil.move(f, f'{broken_dir}/{os.path.basename(f)}')\n",
        "\n",
        "        \n",
        "pool.map(tiling, files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhj7M36j3cP6"
      },
      "source": [
        "# Getting pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2yACnTsxV7k",
        "cellView": "form"
      },
      "source": [
        "#@title download rfr paris model and fix state_dict\n",
        "# rfr paris\n",
        "%cd /content/\n",
        "!gdown --id 1jnUb-EvBw9DcwyWUQyWDdN9o42BPH7uT\n",
        "\n",
        "#https://discuss.pytorch.org/t/dataparallel-changes-parameter-names-issue-with-load-state-dict/60211\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "state_dict = torch.load(\"/content/checkpoint_paris.pth\", map_location='cpu')\n",
        "new_state_dict = OrderedDict()\n",
        "\n",
        "for k, v in state_dict['generator'].items():\n",
        "  if k == 'Pconv1.weight':\n",
        "      name = 'conv1.weight'\n",
        "  elif k == 'Pconv2.weight':\n",
        "      name = 'conv2.weight'\n",
        "  elif k == 'Pconv21.weight':\n",
        "      name = 'conv21.weight'\n",
        "  elif k == 'Pconv22.weight':\n",
        "      name = 'conv22.weight'\n",
        "  else:\n",
        "      name = k\n",
        "\n",
        "  new_state_dict[name] = v\n",
        "\n",
        "torch.save(new_state_dict, '/content/converted.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "txcao4I-2oDV"
      },
      "source": [
        "#@title downloading places2 dfnet\n",
        "%cd /content/\n",
        "!gdown --id 1SGJ_Z9kpchdnZ3Qwwf4HnN-Cq-AeK7vH # dfnet places2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB3Rs436n0WY"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMbSY9iMnaPZ"
      },
      "source": [
        "A summary of all interesting inpainting generators that are not trainable with my code.\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "``Broken generators:``\n",
        "\n",
        "Generators that are not included here since I can't seem to make them work properly:\n",
        "\n",
        "AOT-GAN (2021): [researchmm/AOT-GAN-for-Inpainting](https://github.com/researchmm/AOT-GAN-for-Inpainting)\n",
        "\n",
        "    Couldn't get generator working.\n",
        "\n",
        "PenNet [no AMP] (2019): [researchmm/PEN-Net-for-Inpainting](https://github.com/researchmm/PEN-Net-for-Inpainting/)\n",
        "\n",
        "    Always outputs white for some reason.\n",
        "\n",
        "CRA [no AMP] (2019): [wangyx240/High-Resolution-Image-Inpainting-GAN](https://github.com/wangyx240/High-Resolution-Image-Inpainting-GAN)\n",
        "\n",
        "    Likes to create the color pink.\n",
        "\n",
        "Global [no AMP] (2020): [SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting)\n",
        "\n",
        "    Always outputs white for some reason.\n",
        "\n",
        "crfill (2020): [zengxianyu/crfill](https://github.com/zengxianyu/crfill)\n",
        "\n",
        "    No clear instructions/code result in broken results. Unreleased training code makes a correct implementation harder.\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "``Non-Pytorch generators:``\n",
        "\n",
        "PSR-Net (2020): [sfwyly/PSR-Net](https://github.com/sfwyly/PSR-Net)\n",
        "\n",
        "    Uses Tensorflow 2\n",
        "\n",
        "co-mod-gan (2021): [zsyzzsoft/co-mod-gan](https://github.com/zsyzzsoft/co-mod-gan)\n",
        "\n",
        "    Has a web demo and (a broken link to a) docker. Relies on Tensorflow 1.15 / StyleGAN2 code. A Colab by me for this can be found inside https://github.com/styler00dollar/Colab-co-mod-gan.\n",
        "\n",
        "Diverse-Structure-Inpainting (2021): [USTC-JialunPeng/Diverse-Structure-Inpainting](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)\n",
        "\n",
        "    Tensorflow 1\n",
        "\n",
        "R-MNet (2021): [Jireh-Jam/R-MNet-Inpainting-keras](https://github.com/Jireh-Jam/R-MNet-Inpainting-keras)\n",
        "\n",
        "    Not sure if there is much new and interesting stuff.\n",
        "\n",
        "Hypergraphs (2021): [GouravWadhwa/Hypergraphs-Image-Inpainting](https://github.com/GouravWadhwa/Hypergraphs-Image-Inpainting)\n",
        "\n",
        "    Uses custom conv layer (that is implemented with tensorflow). It sounds interesting, but I got errors when I tried to port it to pytorch.\n",
        "\n",
        "PEPSI (2019): [Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network](https://github.com/Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network)\n",
        "\n",
        "    The net dcpV2 uses.\n",
        "\n",
        "Region (2019): [vickyFox/Region-wise-Inpainting](https://github.com/vickyFox/Region-wise-Inpainting)\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "``Pytorch generators that I never tested:``\n",
        "\n",
        "Edge-LBAM (2021): [wds1998/Edge-LBAM](https://github.com/wds1998/Edge-LBAM)\n",
        "\n",
        "VCNET (2020): [birdortyedi/vcnet-blind-image-inpainting](https://github.com/birdortyedi/vcnet-blind-image-inpainting)\n",
        "\n",
        "    Blind image inpainting without masks.\n",
        "\n",
        "DFMA (2020): [mprzewie/dmfa_inpainting](https://github.com/mprzewie/dmfa_inpainting)\n",
        "\n",
        "GIN (2020): [rlct1/gin-sg](https://github.com/rlct1/gin-sg) and [rlct1/gin](https://github.com/rlct1/gin)\n",
        "\n",
        "StructureFlow (2019): [RenYurui/StructureFlow](https://github.com/RenYurui/StructureFlow)\n",
        "\n",
        "    Needs special files.\n",
        "\n",
        "GMCNN (2018): [shepnerd/inpainting_gmcnn](https://github.com/shepnerd/inpainting_gmcnn)\n",
        "\n",
        "    The net dcpV1 used iirc.\n",
        "\n",
        "ShiftNet (2018): [Zhaoyi-Yan/Shift-Net_pytorch](https://github.com/Zhaoyi-Yan/Shift-Net_pytorch)\n",
        "\n",
        "--------------------------------------------------\n",
        "``Soon:``\n",
        "\n",
        "ICT (2021): [raywzy/ICT](https://github.com/raywzy/ICT)\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "``No training code:``\n",
        "\n",
        "SC-FEGAN (2019): [run-youngjoo/SC-FEGAN](https://github.com/run-youngjoo/SC-FEGAN)\n"
      ]
    }
  ]
}
