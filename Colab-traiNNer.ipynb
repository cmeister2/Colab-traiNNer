{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-traiNNer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY_bDEi1J7bJ"
      },
      "source": [
        "# Colab-traiNNer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My897iQFpTHH"
      },
      "source": [
        "victorca25's BasicSR fork: [victorca25/traiNNer](https://github.com/victorca25/traiNNer)\n",
        "\n",
        "Original colab by [nmkd](https://github.com/n00mkrad) with modifications by [styler00dollar](https://github.com/styler00dollar)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MrEgrBSu4Qu",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "\n",
        "gpu = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "print(\"GPU: \" + gpu[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBbSxWWBOZEI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsK_NAi963qK"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l32IresQs-oW",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "\n",
        "!rm -r \"/content/traiNNer\"\n",
        "!mkdir \"/content/traiNNer\"\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import Image as ipythonimage\n",
        "import os\n",
        "import fileinput\n",
        "import sys\n",
        "\n",
        "sedloc = \"\"\n",
        "%cd /content/\n",
        "#Install apt-fast, for faster installing\n",
        "!/bin/bash -c \"$(curl -sL https://git.io/vokNn)\"\n",
        "#Get some basic dependencies\n",
        "!apt-fast install -y -q -q p7zip-full p7zip-rar\n",
        "\n",
        "# Clone traiNNer\n",
        "!rm -r /content/traiNNer\n",
        "%cd /content/\n",
        "#!git clone \"https://github.com/victorca25/traiNNer.git\"\n",
        "!git clone https://github.com/styler00dollar/Colab-traiNNer traiNNer\n",
        "%cd /content/traiNNer\n",
        "!pip install pytorch_lightning==1.2.5 timm adamp opencv-python tensorboardX pyyaml\n",
        "!pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D\n",
        "\n",
        "# create empty folders\n",
        "!mkdir /content/hr\n",
        "!mkdir /content/lr\n",
        "!mkdir /content/lr_val\n",
        "!mkdir /content/hr_val\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZwoYinu69nB"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRHTppHtsgkr"
      },
      "source": [
        "Pre-configured paths:\n",
        "\n",
        "LR: ```/content/lr```\n",
        "\n",
        "HR: ```/content/hr```\n",
        "\n",
        "LR_VAL: ```/content/lr_val```\n",
        "\n",
        "HR_VAL: ```/content/hr_val```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPkMNWqKgxrR"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu7HxxdGJdir"
      },
      "source": [
        "You need to upload the data and then extract it within colab. You can use Google Drive for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGU28ZJIJlEC"
      },
      "source": [
        "%cd /content/\n",
        "!cp \"/content/drive/MyDrive/animeinterp_mod_datset.tar\" \"/content/data.tar\"\n",
        "!7z x /content/data.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG6BqJD5J41q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6_2VP2MV0eGv"
      },
      "source": [
        "#@title config example\n",
        "%%writefile /content/traiNNer/code/config.yaml\n",
        "name: template\n",
        "scale: 1\n",
        "gpus: 1 # amount of gpus, 0 = cpu\n",
        "distributed_backend: ddp # dp, ddp (for multi-gpu training)\n",
        "tpu_cores: 8 # 8 if you use a Google Colab TPU\n",
        "use_tpu: False\n",
        "use_amp: False\n",
        "use_swa: False\n",
        "progress_bar_refresh_rate: 20\n",
        "default_root_dir: '/content'\n",
        "\n",
        "# Dataset options:\n",
        "datasets:\n",
        "  train:\n",
        "    # DS_inpaint: hr is from dataroot_HR, loads masks\n",
        "    # DS_inpaint_tiled: hr is from dataroot_HR, but images are grids (16x16 256px currently), loads masks\n",
        "    # DS_inpaint_tiled_batch: hr is from dataroot_HR, but images are grids (16x16 256px currently) and processed as a batch (batch_size_DL), loads masks\n",
        "    # DS_lrhr: loads lr from dataroot_LR and hr from dataroot_HR\n",
        "    # DS_lrhr_batch_oft: loads grayscale-hr (3x3 400px) from dataroot_LR and generates lr by downscaling otf randomly\n",
        "\n",
        "    mode: DS_video # DS_video | DS_inpaint | DS_inpaint_tiled | DS_inpaint_tiled_batch | DS_lrhr | DS_lrhr_batch_oft\n",
        "    grayscale: False # If true, reads as 1-Channel image. If false, reads as 3-channel image. Currently only implemented for DS_lrhr_batch_oft\n",
        "    dataroot_HR: '/content/data' # Original, with a single directory. Inpainting will use this directory as source image.\n",
        "    dataroot_LR: '/content/data' # Original, with a single directory\n",
        "    loading_backend: 'PIL' # PIL, OpenCV (currently only in DS_inpaint_tiled_batch)\n",
        "\n",
        "    n_workers: 2 # 0 to disable CPU multithreading, or an integrer representing CPU threads to use for dataloading\n",
        "    batch_size: 1\n",
        "    batch_size_DL: 1\n",
        "    HR_size: 512 # The resolution the network will get. Random crop gets applied if that resolution does not match.\n",
        "    image_channels: 3 # number of channels to load images in\n",
        "\n",
        "    masks: '/content/masks/'\n",
        "    max_epochs: 2000\n",
        "    save_step_frequency: 5000 # also validation frequency\n",
        "\n",
        "    # batch\n",
        "    # If a tiled dataloader is used, specify image charactaristics. Random crop will not be applied. Maybe in the future.\n",
        "    image_size: 512 # Size of one tile\n",
        "    amount_tiles: 2 # Amount of tiles inside the merged grid image\n",
        "\n",
        "    # if edge data is required\n",
        "    canny_min: 100\n",
        "    canny_max: 150\n",
        "\n",
        "  val:\n",
        "    dataroot_HR: '/content/data'\n",
        "    dataroot_LR: '/content/data' # Inpainting will use this directory as input\n",
        "\n",
        "path:\n",
        "    pretrain_model_G:\n",
        "    pretrain_model_D: \n",
        "    checkpoint_path:\n",
        "    checkpoint_save_path: '/content/'\n",
        "    validation_output_path: '/content/val/'\n",
        "    log_path: '/content/logs'\n",
        "\n",
        "# Generator options:\n",
        "network_G:\n",
        "    # CEM (for esrgan, not 1x)\n",
        "    CEM: False # uses hardcoded torch.cuda.FloatTensor\n",
        "    sigmoid_range_limit: False\n",
        "\n",
        "    finetune: False # Important for further rfr/dsnet training. Apply that after training for a while. https://github.com/jingyuanli001/RFR-Inpainting/issues/33\n",
        "\n",
        "    # ESRGAN:\n",
        "    #netG: RRDB_net # RRDB_net (original ESRGAN arch) | MRRDB_net (modified/\"new\" arch)\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64 # of discrim filters in the first conv layer (default: 64, good: 32)\n",
        "    #nb: 23 # (default: 23, good: 8)\n",
        "    #in_nc: 3 # of input image channels: 3 for RGB and 1 for grayscale\n",
        "    #out_nc: 3 # of output image channels: 3 for RGB and 1 for grayscale\n",
        "    #gc: 32\n",
        "    #group: 1\n",
        "    #convtype: Conv2D # Conv2D | PartialConv2D\n",
        "    #net_act: leakyrelu # swish | leakyrelu\n",
        "    #gaussian: false # true | false # esrgan plus, does not work on TPU because of cuda()\n",
        "    #plus: false # true | false\n",
        "    #finalact: None #tanh # Test. Activation function to make outputs fit in [-1, 1] range. Default = None. Coordinate with znorm.\n",
        "    #upsample_mode: 'upconv'\n",
        "    #nr: 3\n",
        "\n",
        "    # ASRGAN:\n",
        "    #which_model_G: asr_resnet # asr_resnet | asr_cnn\n",
        "    #nf: 64\n",
        "\n",
        "    # PPON:\n",
        "    #netG: ppon # | ppon\n",
        "    ##norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 24\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    ##gc: 32\n",
        "    #group: 1\n",
        "    ##convtype: Conv2D #Conv2D | PartialConv2D\n",
        "\n",
        "    # SRGAN:\n",
        "    #netG: sr_resnet # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 16\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "\n",
        "    # SR:\n",
        "    #netG: RRDB_net # RRDB_net | sr_resnet\n",
        "    #norm_type: null\n",
        "    #mode: CNA\n",
        "    #nf: 64\n",
        "    #nb: 23\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    #gc: 32\n",
        "    #group: 1\n",
        "\n",
        "    # PAN:\n",
        "    # netG: pan_net\n",
        "    # in_nc: 3\n",
        "    # out_nc: 3\n",
        "    # nf: 40\n",
        "    # unf: 24\n",
        "    # nb: 16\n",
        "    # self_attention: true\n",
        "    # double_scpa: false\n",
        "\n",
        "    # edge-informed-sisr\n",
        "    #which_model_G: sisr\n",
        "    #use_spectral_norm: True\n",
        "\n",
        "    # USRNet\n",
        "    #netG: USRNet\n",
        "    #in_nc=4\n",
        "    #out_nc=3\n",
        "    #nc=[64, 128, 256, 512]\n",
        "    #nb=2\n",
        "    #act_mode='R'\n",
        "    #downsample_mode='strideconv'\n",
        "    #upsample_mode='convtranspose'\n",
        "\n",
        "    # GLEAN (2021)\n",
        "    # Warning: Does require \"pip install mmcv-full\"\n",
        "    #netG: GLEAN\n",
        "    #in_size: 512\n",
        "    #out_size: 512\n",
        "    #img_channels: 4\n",
        "    #img_channels_out: 3\n",
        "    #rrdb_channels: 16 # 64\n",
        "    #num_rrdbs: 8 # 23\n",
        "    #style_channels: 512 # 512\n",
        "    #num_mlps: 4 # 8\n",
        "    #channel_multiplier: 2\n",
        "    #blur_kernel: [1, 3, 3, 1]\n",
        "    #lr_mlp: 0.01\n",
        "    #default_style_mode: 'mix'\n",
        "    #eval_style_mode: 'single'\n",
        "    #mix_prob: 0.9\n",
        "    #pretrained: False # only works with official settings\n",
        "    #bgr2rgb: False\n",
        "\n",
        "    # srflow (upscaling factors: 4, 8, 16)\n",
        "    # Warning: Can be very unstable with batch_size 1, use higher batch_size\n",
        "    #netG: srflow\n",
        "    #in_nc: 3\n",
        "    #out_nc: 3\n",
        "    #nf: 64\n",
        "    #nb: 23\n",
        "    #train_RRDB: false\n",
        "    #train_RRDB_delay: 0.5\n",
        "    #flow:\n",
        "    #  K: 16\n",
        "    #  L: 3\n",
        "    #  noInitialInj: true\n",
        "    #  coupling: CondAffineSeparatedAndCond\n",
        "    #  additionalFlowNoAffine: 2\n",
        "    #  split:\n",
        "    #    enable: true\n",
        "    #  fea_up0: true\n",
        "    #  stackRRDB:\n",
        "    #    blocks: [ 1, 8, 15, 22 ]\n",
        "    #    concat: true\n",
        "    #nll_weight: 1\n",
        "    #freeze_iter: 100000\n",
        "\n",
        "    # DFDNet\n",
        "    # Warning: Expects \"DictionaryCenter512\" in the current folder, you can get the data here: https://drive.google.com/drive/folders/1bayYIUMCSGmoFPyd4Uu2Uwn347RW-vl5\n",
        "    # Also wants a folder called \"landmarks\", you can generate that data yourself. Example: https://github.com/styler00dollar/Colab-DFDNet/blob/local/Colab-DFDNet-lightning-train.ipynb\n",
        "    # Hardcoded resolution: 512px\n",
        "    #netG: DFDNet\n",
        "    #dictionary_path: \"/content/DictionaryCenter512\"\n",
        "    #landmarkpath: \"/content/landmarks\"\n",
        "    #val_landmarkpath: \"/content/landmarks\"\n",
        "\n",
        "    # GFPGAN (2021) [EXPERIMENTAL]\n",
        "    # does require ninja\n",
        "    # because it compiles files, the startup time is quite long\n",
        "    #netG: GFPGAN\n",
        "    #input_channels: 4\n",
        "    #output_channels: 3\n",
        "    #out_size: 512\n",
        "    #num_style_feat: 512\n",
        "    #channel_multiplier: 1\n",
        "    #resample_kernel: [1, 3, 3, 1]\n",
        "    #decoder_load_path: # None\n",
        "    #fix_decoder: True\n",
        "    #num_mlp: 8\n",
        "    #lr_mlp: 0.01\n",
        "    #input_is_latent: False\n",
        "    #different_w: False\n",
        "    #narrow: 1\n",
        "    #sft_half: False\n",
        "\n",
        "    # GPEN\n",
        "    # does require ninja\n",
        "    # because it compiles files, the startup time is quite long\n",
        "    # output_channels is hardcoded to 3\n",
        "    #netG: GPEN\n",
        "    #input_channels: 4\n",
        "    #size: 512\n",
        "    #style_dim: 512\n",
        "    #n_mlp: 8\n",
        "    #channel_multiplier: 2\n",
        "    #blur_kernel: [1, 3, 3, 1]\n",
        "    #lr_mlp: 0.01\n",
        "    #pooling: True # Experimental, to have any input size\n",
        "\n",
        "    # comodgan (2021)\n",
        "    # needs ninja\n",
        "    # because it compiles files, the startup time is quite long\n",
        "    #netG: comodgan\n",
        "    #dlatent_size: 512\n",
        "    #num_channels: 3 # amount of channels without mask\n",
        "    #resolution: 512\n",
        "    #fmap_base: 16384 # 16 << 10\n",
        "    #fmap_decay: 1.0\n",
        "    #fmap_min: 1\n",
        "    #fmap_max: 512\n",
        "    #randomize_noise: True\n",
        "    #architecture: 'skip'\n",
        "    #nonlinearity: 'lrelu'\n",
        "    #resample_kernel: [1,3,3,1]\n",
        "    #fused_modconv: True\n",
        "    #pix2pix: False\n",
        "    #dropout_rate: 0.5\n",
        "    #cond_mod: True\n",
        "    #style_mod: True\n",
        "    #noise_injection: True\n",
        "\n",
        "    # ----Inpainting Generators----\n",
        "    # DFNet (batch_size: 2+, needs 2^x image input and validation) (2019)\n",
        "    #netG: DFNet\n",
        "    #c_img: 3\n",
        "    #c_mask: 1\n",
        "    #c_alpha: 3\n",
        "    #mode: nearest\n",
        "    #norm: batch\n",
        "    #act_en: relu\n",
        "    #act_de: leaky_relu\n",
        "    #en_ksize: [7, 5, 5, 3, 3, 3, 3, 3]\n",
        "    #de_ksize: [3, 3, 3, 3, 3, 3, 3, 3]\n",
        "    #blend_layers: [0, 1, 2, 3, 4, 5]\n",
        "    #conv_type: normal # partial | normal | deform\n",
        "    \n",
        "\n",
        "    # EdgeConnect (2019)\n",
        "    #netG: EdgeConnect\n",
        "    #use_spectral_norm: True\n",
        "    #residual_blocks_edge: 8\n",
        "    #residual_blocks_inpaint: 8\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #conv_type_inpaint: 'normal' # normal | partial | deform\n",
        "\n",
        "    # CSA (2019)\n",
        "    #netG: CSA\n",
        "    #c_img: 3\n",
        "    #norm: 'instance'\n",
        "    #act_en: 'leaky_relu'\n",
        "    #act_de: 'relu'\n",
        "\n",
        "    # RN (2020)\n",
        "    #netG: RN\n",
        "    #input_channels: 3\n",
        "    #residual_blocks: 8\n",
        "    #threshold: 0.8\n",
        "\n",
        "    # deepfillv1 (2018)\n",
        "    #netG:  deepfillv1\n",
        "\n",
        "    # deepfillv2 (2019)\n",
        "    #netG: deepfillv2\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu'\n",
        "    #norm: 'in'\n",
        "    #conv_type: partial # partial | normal\n",
        "\n",
        "    # Adaptive (2020)\n",
        "    #netG: Adaptive\n",
        "    #in_channels: 3\n",
        "    #residual_blocks: 1\n",
        "    #init_weights: True\n",
        "\n",
        "    # Global (2020)\n",
        "    #netG: Global\n",
        "    #input_dim: 5\n",
        "    #ngf: 32\n",
        "    #use_cuda: True\n",
        "    #device_ids: [0]\n",
        "\n",
        "    # Pluralistic (2019)\n",
        "    #netG: Pluralistic\n",
        "    #ngf_E: 32\n",
        "    #z_nc_E: 128\n",
        "    #img_f_E: 128\n",
        "    #layers_E: 5\n",
        "    #norm_E: 'none'\n",
        "    #activation_E: 'LeakyReLU'\n",
        "    #ngf_G: 32\n",
        "    #z_nc_G: 128\n",
        "    #img_f_G: 128\n",
        "    #L_G: 0\n",
        "    #output_scale_G: 1\n",
        "    #norm_G: 'instance'\n",
        "    #activation_G: 'LeakyReLU'\n",
        "\n",
        "    # crfill (2020)\n",
        "    #netG: crfill\n",
        "    #cnum: 48\n",
        "\n",
        "    # DeepDFNet (experimental)\n",
        "    #netG: DeepDFNet\n",
        "    #in_channels:  4\n",
        "    #out_channels:  3\n",
        "    #latent_channels:  64\n",
        "    #pad_type:  'zero'\n",
        "    #activation:  'lrelu'\n",
        "    #norm: 'in'\n",
        "\n",
        "    # partial (2018)\n",
        "    #netG: partial\n",
        "\n",
        "    # DMFN (2020)\n",
        "    #netG: DMFN\n",
        "    #in_nc: 4\n",
        "    #out_nc: 3\n",
        "    #nf: 64\n",
        "    #n_res: 8\n",
        "    #norm: 'in'\n",
        "    #activation: 'relu'\n",
        "\n",
        "    # pennet (2019)\n",
        "    #netG: pennet\n",
        "\n",
        "    # LBAM (2019)\n",
        "    #netG: LBAM\n",
        "    #inputChannels: 4\n",
        "    #outputChannels: 3\n",
        "\n",
        "    # RFR (use_swa: false, no TPU) (2020)\n",
        "    #netG: RFR\n",
        "    #conv_type: partial # partial | deform\n",
        "\n",
        "    # FRRN (2019)\n",
        "    #netG: FRRN\n",
        "\n",
        "    # PRVS (2019)\n",
        "    #netG: PRVS\n",
        "\n",
        "    # CRA (HR_size: 512) (2020)\n",
        "    #netG: CRA\n",
        "    #activation: 'elu'\n",
        "    #norm: 'none'\n",
        "\n",
        "    # atrous (2020)\n",
        "    #netG: atrous\n",
        "\n",
        "    # MEDFE (batch_size: 1) (2020)\n",
        "    #netG: MEDFE\n",
        "\n",
        "    # AdaFill (2021)\n",
        "    #netG: AdaFill\n",
        "\n",
        "    # lightweight_gan (2021)\n",
        "    #netG: lightweight_gan\n",
        "    #image_size: 512\n",
        "    #latent_dim: 256\n",
        "    #fmap_max: 512\n",
        "    #fmap_inverse_coef: 12\n",
        "    #transparent: False\n",
        "    #greyscale: False\n",
        "    #freq_chan_attn: False\n",
        "\n",
        "\n",
        "    # ----Interpolation Generators----\n",
        "    netG: CAIN\n",
        "    depth: 3\n",
        "\n",
        "# Discriminator options:\n",
        "network_D:\n",
        "    d_loss_fool_weight: 1 # inside the generator loop, trying to fool the disciminator\n",
        "    d_loss_weight: 1 # inside own discriminator update\n",
        "    \n",
        "    #netD: # in case there is no discriminator, leave it empty\n",
        "\n",
        "    # VGG\n",
        "    #netD: VGG\n",
        "    #size: 256\n",
        "    #in_nc: 3 #3\n",
        "    #base_nf: 64\n",
        "    #norm_type: 'batch'\n",
        "    #act_type: 'leakyrelu'\n",
        "    #mode: 'CNA'\n",
        "    #convtype: 'Conv2D'\n",
        "    #arch: 'ESRGAN'\n",
        "\n",
        "    # VGG fea\n",
        "    #netD: VGG_fea\n",
        "    #size: 256\n",
        "    #in_nc: 3\n",
        "    #base_nf: 64\n",
        "    #norm_type: 'batch'\n",
        "    #act_type: 'leakyrelu'\n",
        "    #mode: 'CNA'\n",
        "    #convtype: 'Conv2D'\n",
        "    #arch: 'ESRGAN'\n",
        "    #spectral_norm: False\n",
        "    #self_attention: False\n",
        "    #max_pool: False\n",
        "    #poolsize: 4\n",
        "\n",
        "\n",
        "    #netD: VGG_128_SN\n",
        "\n",
        "    # VGGFeatureExtractor\n",
        "    #netD: VGGFeatureExtractor\n",
        "    #feature_layer: 34\n",
        "    #use_bn: False\n",
        "    #use_input_norm: True\n",
        "    #device: 'cpu'\n",
        "    #z_norm: False\n",
        "\n",
        "    # PatchGAN\n",
        "    #netD: NLayerDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #n_layers: 3\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "    #use_sigmoid: False\n",
        "    #getIntermFeat: False\n",
        "    #patch: True\n",
        "    #use_spectral_norm: False\n",
        "\n",
        "    # Multiscale\n",
        "    #netD: MultiscaleDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #n_layers: 3\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "    #se_sigmoid: False\n",
        "    #num_D: 3\n",
        "    #getIntermFeat: False\n",
        "\n",
        "    #netD: ResNet101FeatureExtractor\n",
        "    #use_input_norm: True\n",
        "    #device: 'cpu'\n",
        "    #z_norm: False\n",
        "\n",
        "    # MINC\n",
        "    #netD: MINCNet\n",
        "\n",
        "    # Pixel\n",
        "    #netD: PixelDiscriminator\n",
        "    #input_nc: 3\n",
        "    #ndf: 64\n",
        "    #norm_layer: nn.BatchNorm2d\n",
        "\n",
        "    # EfficientNet (3-channel input)\n",
        "    #netD: EfficientNet\n",
        "    #EfficientNet_pretrain: 'efficientnet-b0'\n",
        "    #num_classes: 1 # should be 1\n",
        "\n",
        "    # ResNeSt (not working)\n",
        "    #netD: ResNeSt\n",
        "    #ResNeSt_pretrain: 'resnest50' # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "    #pretrained: False # cant be true currently\n",
        "    #num_classes: 1\n",
        "\n",
        "    # Transformer (not working)\n",
        "    #netD: TranformerDiscriminator\n",
        "    #img_size: 256\n",
        "    #patch_size: 1\n",
        "    #in_chans: 3\n",
        "    #num_classes: 1\n",
        "    #embed_dim: 64\n",
        "    #depth: 7\n",
        "    #num_heads: 4\n",
        "    #mlp_ratio: 4.\n",
        "    #qkv_bias: False\n",
        "    #qk_scale: None\n",
        "    #drop_rate: 0.\n",
        "    #attn_drop_rate: 0.\n",
        "    #drop_path_rate: 0.\n",
        "    #hybrid_backbone: None\n",
        "    #norm_layer: \n",
        "\n",
        "    # context_encoder (num_classes can't be set, broadcasting warning will be shown, training works, but I am not sure if it will work correctly)\n",
        "    #netD: context_encoder\n",
        "\n",
        "    # Transformer (doesn't do init)\n",
        "    #netD: ViT\n",
        "    #image_size: 256\n",
        "    #patch_size: 32\n",
        "    #num_classes: 1\n",
        "    #dim: 1024\n",
        "    #depth: 6\n",
        "    #heads: 16\n",
        "    #mlp_dim: 2048\n",
        "    #dropout: 0.1\n",
        "    #emb_dropout: 0.1\n",
        "\n",
        "    # Transformer (doesn't do init)\n",
        "    #netD: DeepViT\n",
        "    #image_size: 256\n",
        "    #patch_size: 32\n",
        "    #num_classes: 1\n",
        "    #dim: 1024\n",
        "    #depth: 6\n",
        "    #heads: 16\n",
        "    #mlp_dim: 2048\n",
        "    #dropout: 0.1\n",
        "    #emb_dropout: 0.1\n",
        "\n",
        "    # RepVGG\n",
        "    #netD: RepVGG\n",
        "    #RepVGG_arch: RepVGG-A0 # RepVGG-A0, RepVGG-A1, RepVGG-A2, RepVGG-B0, RepVGG-B1, RepVGG-B1g2, RepVGG-B1g4, , RepVGG-B2, RepVGG-B2g2, RepVGG-B2g4, RepVGG-B3, RepVGG-B3g2, RepVGG-B3g4\n",
        "    #num_classes: 1\n",
        "\n",
        "    # squeezenet\n",
        "    #netD: squeezenet\n",
        "    #version: \"1_1\" # 1_0, 1_1\n",
        "    #num_classes: 1\n",
        "\n",
        "    # SwinTransformer (doesn't do init)\n",
        "    #netD: SwinTransformer\n",
        "    #hidden_dim: 96\n",
        "    #layers: [2, 2, 6, 2]\n",
        "    #heads: [3, 6, 12, 24]\n",
        "    #channels: 3\n",
        "    #num_classes: 1\n",
        "    #head_dim: 32\n",
        "    #window_size: 8\n",
        "    #downscaling_factors: [4, 2, 2, 2]\n",
        "    #relative_pos_embedding: True\n",
        "\n",
        "    # mobilenetV3 (doesn't do init)\n",
        "    #netD: mobilenetV3\n",
        "    #mode: small # small, large\n",
        "    #n_class: 1\n",
        "    #input_size: 256\n",
        "\n",
        "    # resnet\n",
        "    #netD: resnet\n",
        "    #resnet_arch: resnet50 # resnet50, resnet101, resnet152\n",
        "    #num_classes: 1\n",
        "    #pretrain: True\n",
        "  \n",
        "    # NFNet\n",
        "    #netD: NFNet\n",
        "    #num_classes: 1\n",
        "    #variant: 'F0'         # F0 - F7\n",
        "    #stochdepth_rate: 0.25 # 0-1, the probability that a layer is dropped during one step\n",
        "    #alpha: 0.2            # Scaling factor at the end of each block\n",
        "    #se_ratio: 0.5         # Squeeze-Excite expansion ratio\n",
        "    #activation: 'gelu'    # or 'relu'\n",
        "\n",
        "    # lvvit (2021)\n",
        "    # Warning: Needs 'pip install timm==0.4.5'\n",
        "    #netD: lvvit\n",
        "    #img_size: 224\n",
        "    #patch_size: 16\n",
        "    #in_chans: 3\n",
        "    #num_classes: 1\n",
        "    #embed_dim: 768\n",
        "    #depth: 12\n",
        "    #num_heads: 12\n",
        "    #mlp_ratio: 4.\n",
        "    #qkv_bias: False\n",
        "    #qk_scale: # None\n",
        "    #drop_rate: 0.\n",
        "    #attn_drop_rate: 0.\n",
        "    #drop_path_rate: 0.\n",
        "    #drop_path_decay: 'linear'\n",
        "    #hybrid_backbone: # None\n",
        "    ##norm_layer: nn.LayerNorm # Deafault: nn.LayerNorm / can't be configured\n",
        "    #p_emb: '4_2'\n",
        "    #head_dim: # None\n",
        "    #skip_lam: 1.0\n",
        "    #order: # None\n",
        "    #mix_token: False\n",
        "    #return_dense: False\n",
        "\n",
        "    # timm\n",
        "    # pip install timm\n",
        "    # you can loop up models here: https://rwightman.github.io/pytorch-image-models/\n",
        "    #netD: timm\n",
        "    #timm_model: \"tf_efficientnetv2_b0\"\n",
        "\n",
        "    netD: resnet3d\n",
        "    model_depth: 50 # [10, 18, 34, 50, 101, 152, 200]\n",
        "\n",
        "train: \n",
        "    scheduler: AdamP # Adam, AdamP, Adam, SGDP, MADGRAD, cosangulargrad [maybe broken], tanangulargrad [maybe broken]\n",
        "    lr: 0.000001\n",
        "    \n",
        "    # AdamP, AGDP, MADGRAD, cosangulargrad, tanangulargrad\n",
        "    weight_decay: 0.01\n",
        "\n",
        "    # SGDP, MAGDRAD\n",
        "    momentum: 0.9\n",
        "\n",
        "    # AdamP, cosangulargrad, tanangulargrad\n",
        "    betas0: 0.9\n",
        "    betas1: 0.999\n",
        "    \n",
        "    # SGDP\n",
        "    nesterov: True\n",
        "\n",
        "    # MADGRAD, cosangulargrad, tanangulargrad\n",
        "    eps: 1e-6\n",
        "\n",
        "    # Losses:\n",
        "    L1Loss_weight: 0\n",
        "\n",
        "    # HFENLoss\n",
        "    HFEN_weight: 0\n",
        "    loss_f: L1CosineSim # L1Loss | L1CosineSim\n",
        "    kernel: 'log'\n",
        "    kernel_size: 15\n",
        "    sigma: 2.5\n",
        "    norm: False\n",
        "\n",
        "    # Elastic\n",
        "    Elatic_weight: 0\n",
        "    a: 0.2\n",
        "    reduction_elastic: 'mean'\n",
        "\n",
        "    # Relative L1\n",
        "    Relative_l1_weight: 0\n",
        "    eps: .01\n",
        "    reduction_realtive: 'mean'\n",
        "\n",
        "    # L1CosineSim (3-channel input)\n",
        "    L1CosineSim_weight: 0\n",
        "    loss_lambda: 5\n",
        "    reduction_L1CosineSim: 'mean'\n",
        "\n",
        "    # ClipL1\n",
        "    ClipL1_weight: 0\n",
        "    clip_min: 0.0\n",
        "    clip_max: 10.0\n",
        "\n",
        "    # FFTLoss\n",
        "    FFTLoss_weight: 1\n",
        "    loss_f_fft: L1Loss\n",
        "    reduction_fft: 'mean'\n",
        "\n",
        "    OFLoss_weight: 0\n",
        "\n",
        "    # GPLoss\n",
        "    GPLoss_weight: 0\n",
        "    trace: False\n",
        "    spl_denorm: False\n",
        "\n",
        "    # CPLoss\n",
        "    CPLoss_weight: 0\n",
        "    rgb: True\n",
        "    yuv: True\n",
        "    yuvgrad: True\n",
        "    trace: False\n",
        "    spl_denorm: False\n",
        "    yuv_denorm: False\n",
        "\n",
        "    StyleLoss_weight: 0\n",
        "\n",
        "    # TVLoss\n",
        "    TVLoss_weight: 0.00001\n",
        "    tv_type: 'tv'\n",
        "    p: 1\n",
        "\n",
        "    # Contextual_Loss (3-channel input)\n",
        "    Contexual_weight: 0\n",
        "    crop_quarter: False\n",
        "    max_1d_size: 100\n",
        "    distance_type: 'cosine' # [\"11\", \"l2\", \"consine\"]\n",
        "    b: 1.0\n",
        "    band_width: 0.5\n",
        "    # for vgg\n",
        "    use_vgg: False\n",
        "    net_contextual: 'vgg19'\n",
        "    layers_weights: {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    # for timm\n",
        "    use_timm: True\n",
        "    timm_model: \"tf_efficientnetv2_b0\"\n",
        "    # for both\n",
        "    calc_type: 'regular' # [\"bilateral\" | \"symetric\" | None]\n",
        "\n",
        "    # Style (3-channel input)\n",
        "    StyleLoss_weight: 0\n",
        "\n",
        "    # PerceptualLoss\n",
        "    perceptual_weight: 1\n",
        "    net: PNetLin # PNetLin, DSSIM (?)\n",
        "    pnet_type: 'vgg' # alex, squeeze, vgg\n",
        "    pnet_rand: False\n",
        "    pnet_tune: False\n",
        "    use_dropout: True\n",
        "    spatial: False\n",
        "    version: '0.1' # only version\n",
        "    lpips: True\n",
        "\n",
        "    # only if the network outputs 2 images, will use l1\n",
        "    stage1_weight: 0 \n",
        "\n",
        "    # Differentiable Augmentation for Data-Efficient GAN Training\n",
        "    diffaug: False\n",
        "    policy: 'color,translation,cutout'\n",
        "\n",
        "    # Metrics\n",
        "    metrics: [] # PSNR | SSIM | AE | MSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWaG4UHJrTye",
        "cellView": "both"
      },
      "source": [
        "%cd '/content/traiNNer/code'\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}